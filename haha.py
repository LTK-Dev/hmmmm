import streamlit as st
import pandas as pd
import os
import google.generativeai as genai
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum
import json

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho FAISS v√† LangChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate

# --- 1. T·ªêI ∆ØU HI·ªÜU NƒÇNG V·ªöI CACHING ---
@st.cache_resource
def get_embedder():
    """T·∫£i v√† cache m√¥ h√¨nh embedding."""
    print("INFO: ƒêang t·∫£i m√¥ h√¨nh embedding...")
    return SentenceTransformerEmbeddings(
        model_name=EMBEDDER_MODEL,
        model_kwargs={'device': 'cpu'}
    )

# --- Data Classes v√† Enums ---
class AgentType(Enum):
    ROUTER = "router"
    PRODUCT_SPECIALIST = "product_specialist"
    GENERAL_CONSULTANT = "general_consultant"

@dataclass
class AgentResponse:
    content: str
    confidence: float
    agent_type: AgentType
    metadata: Dict[str, Any] = None

@dataclass
class TaskRequest:
    query: str
    context: List[str] = None
    metadata: Dict[str, Any] = None

# --- C·∫•u h√¨nh ---
PRODUCT_CSV_FILE = 'EKS.csv'
SCRIPT_CSV_FILE = 'EKS_ques.csv'
PRODUCT_FAISS_PATH = "faiss_index_product"
SCRIPT_FAISS_PATH = "faiss_index_script"
EMBEDDER_MODEL = 'intfloat/multilingual-e5-base'
GENERATIVE_MODEL = 'gemini-2.0-flash'

# --- Base Agent Class ---
class BaseAgent:
    def __init__(self, agent_type: AgentType, model: genai.GenerativeModel):
        self.agent_type = agent_type
        self.model = model
        self.name = agent_type.value
    
    def process(self, request: TaskRequest) -> AgentResponse:
        """Base method ƒë·ªÉ x·ª≠ l√Ω request - s·∫Ω ƒë∆∞·ª£c override b·ªüi c√°c agent con"""
        raise NotImplementedError("Subclasses must implement process method")
    
    def _generate_stream_response(self, prompt: str):
        """Helper method ƒë·ªÉ generate streaming response"""
        try:
            response_stream = self.model.generate_content(prompt, stream=True)
            for chunk in response_stream:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            st.error(f"L·ªói khi generate response t·ª´ {self.name}: {e}")
            yield f"Xin l·ªói, {self.name} g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu."

# --- Router Agent ---
# --- Router Agent (PHI√äN B·∫¢N N√ÇNG C·∫§P) ---
# --- Router Agent (PHI√äN B·∫¢N T·ªîNG QU√ÅT) ---
# --- Router Agent (PHI√äN B·∫¢N CH·ªêNG L·ªñI FORMAT) ---
class RouterAgent(BaseAgent):
    def __init__(self, model: genai.GenerativeModel):
        super().__init__(AgentType.ROUTER, model)
        # PROMPT SI√äU CH·∫∂T CH·∫º ƒê·ªÇ ƒê·∫¢M B·∫¢O CH·ªà TR·∫¢ V·ªÄ JSON
        self.routing_prompt = """
        B·∫°n l√† m·ªôt API ph√¢n lo·∫°i, kh√¥ng ph·∫£i l√† m·ªôt tr·ª£ l√Ω tr√≤ chuy·ªán.
        Nhi·ªám v·ª• DUY NH·∫§T c·ªßa b·∫°n l√† nh·∫≠n ƒë·∫ßu v√†o v√† tr·∫£ v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng JSON.

        **QUY T·∫ÆC B·∫ÆT BU·ªòC:**
        1.  **CH·ªà** tr·∫£ l·ªùi b·∫±ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON h·ª£p l·ªá.
        2.  C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n **PH·∫¢I** b·∫Øt ƒë·∫ßu b·∫±ng d·∫•u `{` v√† k·∫øt th√∫c b·∫±ng d·∫•u `}`.
        3.  **KH√îNG** ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o tr∆∞·ªõc ho·∫∑c sau ƒë·ªëi t∆∞·ª£ng JSON.
        4.  **KH√îNG** ƒë∆∞·ª£c gi·∫£i th√≠ch.
        5.  **KH√îNG** ƒë∆∞·ª£c s·ª≠ d·ª•ng markdown (v√≠ d·ª•: ```json).

        **LOGIC PH√ÇN LO·∫†I:**
        - B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c m·ªôt "C√¢u h·ªèi" v√† m·ªôt "G·ª£i √Ω".
        - **∆Øu ti√™n cao nh·∫•t cho "G·ª£i √Ω"**:
            - N·∫øu G·ª£i √Ω ch·ª©a "product_specialist", h√£y ch·ªçn `product_specialist`.
            - N·∫øu G·ª£i √Ω ch·ª©a "general_consultant", h√£y ch·ªçn `general_consultant`.
        - N·∫øu c√¢u h·ªèi y√™u c·∫ßu "so s√°nh" ho·∫∑c h·ªèi v·ªÅ "ch√≠nh s√°ch", h√£y ch·ªçn `general_consultant`.

        **ƒê·∫¶U V√ÄO:**
        - G·ª£i √Ω: "{hint}"
        - C√¢u h·ªèi: "{query}"

        **ƒê·∫¶U RA (CH·ªà JSON):**
        ```json
        {{
            "agent": "product_specialist" ho·∫∑c "general_consultant",
            "confidence": 1.0,
            "reasoning": "L√Ω do ng·∫Øn g·ªçn d·ª±a tr√™n G·ª£i √Ω v√† C√¢u h·ªèi."
        }}
        ```
        """

    def process(self, request: TaskRequest, hint: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch v√† route c√¢u h·ªèi, c√≥ s·ª≠ d·ª•ng hint t·ª´ pre-check."""
        try:
            prompt = self.routing_prompt.format(query=request.query, hint=hint)
            response = self.model.generate_content(prompt)

            # Code v·∫´n gi·ªØ l·∫°i b∆∞·ªõc clean ƒë·ªÉ ph√≤ng tr∆∞·ªùng h·ª£p hy h·ªØu
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "")
            routing_decision = json.loads(cleaned_text)

            return {
                'target_agent': routing_decision.get('agent', 'general_consultant'),
                'confidence': routing_decision.get('confidence', 0.5),
                'reasoning': routing_decision.get('reasoning', 'Ph√¢n t√≠ch t·ª± ƒë·ªông')
            }
        except (json.JSONDecodeError, AttributeError) as e:
            st.warning(f"Router Agent tr·∫£ v·ªÅ format kh√¥ng ƒë√∫ng, s·ª≠ d·ª•ng fallback logic. L·ªói: {e}")
            # Fallback logic ƒë∆∞·ª£c gi·ªØ l·∫°i nh∆∞ m·ªôt l·ªõp b·∫£o v·ªá cu·ªëi c√πng
            if "product_specialist" in hint:
                 return {'target_agent': 'product_specialist', 'confidence': 0.7, 'reasoning': 'Fallback d·ª±a tr√™n hint'}
            else:
                 return {'target_agent': 'general_consultant', 'confidence': 0.7, 'reasoning': 'Fallback d·ª±a tr√™n hint'}
        except Exception as e:
            st.error(f"L·ªói nghi√™m tr·ªçng t·∫°i Router Agent: {e}")
            return {'target_agent': 'general_consultant', 'confidence': 0.5, 'reasoning': 'Fallback do l·ªói h·ªá th·ªëng'}

# --- Product Specialist Agent ---
class ProductSpecialistAgent(BaseAgent):
    def __init__(self, model: genai.GenerativeModel, vector_store: FAISS):
        super().__init__(AgentType.PRODUCT_SPECIALIST, model)
        self.vector_store = vector_store
        self.specialist_prompt = """
        B·∫°n l√† Product Specialist Agent c·ªßa EKS - chuy√™n gia s√¢u v·ªÅ c√°c s·∫£n ph·∫©m m·ªπ ph·∫©m. 
        B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o c∆° s·ªü d·ªØ li·ªáu chi ti·∫øt v·ªÅ t·∫•t c·∫£ s·∫£n ph·∫©m EKS.

        **Th√¥ng tin s·∫£n ph·∫©m t·ª´ database:**
        ---
        {context}
        ---

        **C√¢u h·ªèi t·ª´ kh√°ch h√†ng:**
        {query}

        **Chuy√™n m√¥n c·ªßa b·∫°n:**
        - Ph√¢n t√≠ch th√†nh ph·∫ßn chi ti·∫øt
        - Gi·∫£i th√≠ch c∆° ch·∫ø ho·∫°t ƒë·ªông
        - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ch√≠nh x√°c
        - C·∫£nh b√°o ch·ªëng ch·ªâ ƒë·ªãnh
        - T∆∞ v·∫•n b·∫£o qu·∫£n v√† l∆∞u √Ω

        **C√°ch tr·∫£ l·ªùi:**
        1. **X∆∞ng h√¥**: "T√¥i l√† chuy√™n gia s·∫£n ph·∫©m EKS"
        2. **Phong c√°ch**: Chuy√™n nghi·ªáp, chi ti·∫øt, d·ª±a tr√™n khoa h·ªçc
        3. **C·∫•u tr√∫c**: S·ª≠ d·ª•ng bullet points v√† headers ƒë·ªÉ d·ªÖ ƒë·ªçc
        4. **Tr√≠ch d·∫´n**: Lu√¥n n√™u r√µ t√™n s·∫£n ph·∫©m c·ª• th·ªÉ
        5. **ƒê·ªô tin c·∫≠y**: Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n data c√≥ s·∫µn, th·ª´a nh·∫≠n n·∫øu kh√¥ng c√≥ th√¥ng tin

        **L∆∞u √Ω quan tr·ªçng**: N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong database, 
        h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ n√†y trong c∆° s·ªü d·ªØ li·ªáu s·∫£n ph·∫©m EKS."
        """
    
    def process(self, request: TaskRequest) -> AgentResponse:
        # Retrieve relevant product information
        context = self._get_product_context(request.query)
        
        return AgentResponse(
            content="", # S·∫Ω ƒë∆∞·ª£c fill b·ªüi streaming
            confidence=0.9,
            agent_type=self.agent_type,
            metadata={'context_size': len(context), 'source': 'product_database'}
        )
    
    def process_stream(self, request: TaskRequest):
        """Streaming version c·ªßa process method"""
        context = self._get_product_context(request.query)
        prompt = self.specialist_prompt.format(context="\n\n".join(context), query=request.query)
        
        yield from self._generate_stream_response(prompt)
    
    def _get_product_context(self, query: str, k: int = 5) -> List[str]:
        """L·∫•y context t·ª´ product vector store"""
        try:
            results = self.vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in results]
        except Exception as e:
            st.error(f"L·ªói khi truy xu·∫•t th√¥ng tin s·∫£n ph·∫©m: {e}")
            return []

# --- General Consultant Agent ---
class GeneralConsultantAgent(BaseAgent):
    def __init__(self, model: genai.GenerativeModel, vector_store: FAISS):
        super().__init__(AgentType.GENERAL_CONSULTANT, model)
        self.vector_store = vector_store
        self.consultant_prompt = """
        B·∫°n l√† General Consultant Agent c·ªßa EKS - t∆∞ v·∫•n vi√™n chuy√™n v·ªÅ th√¥ng tin t·ªïng qu√°t, 
        so s√°nh s·∫£n ph·∫©m v√† d·ªãch v·ª• kh√°ch h√†ng.

        **Th√¥ng tin tham kh·∫£o t·ª´ k·ªãch b·∫£n Q&A:**
        ---
        {context}
        ---

        **C√¢u h·ªèi t·ª´ kh√°ch h√†ng:**
        {query}

        **Chuy√™n m√¥n c·ªßa b·∫°n:**
        - So s√°nh v√† ƒë√°nh gi√° s·∫£n ph·∫©m
        - Th√¥ng tin v·ªÅ th∆∞∆°ng hi·ªáu EKS
        - Ch√≠nh s√°ch v√† d·ªãch v·ª•
        - H∆∞·ªõng d·∫´n mua h√†ng v√† li√™n h·ªá
        - T∆∞ v·∫•n l·ª±a ch·ªçn ph√π h·ª£p

        **C√°ch tr·∫£ l·ªùi:**
        1. **X∆∞ng h√¥**: "T√¥i l√† t∆∞ v·∫•n vi√™n EKS"
        2. **Phong c√°ch**: Th√¢n thi·ªán, d·ªÖ hi·ªÉu, t·∫≠p trung v√†o gi·∫£i ph√°p
        3. **∆Øu ti√™n**: N·∫øu c√≥ s·∫µn c√¢u tr·∫£ l·ªùi trong k·ªãch b·∫£n, s·ª≠ d·ª•ng y nguy√™n
        4. **Linh ho·∫°t**: C√≥ th·ªÉ tham kh·∫£o ki·∫øn th·ª©c chung n·∫øu c·∫ßn
        5. **H√†nh ƒë·ªông**: ƒê∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ v√† b∆∞·ªõc ti·∫øp theo

        **ƒê·∫∑c bi·ªát**: N·∫øu t√¨m th·∫•y c√¢u h·ªèi t∆∞∆°ng t·ª± trong k·ªãch b·∫£n, 
        h√£y s·ª≠ d·ª•ng ch√≠nh x√°c c√¢u tr·∫£ l·ªùi ƒë√≥.
        """
    
    def process(self, request: TaskRequest) -> AgentResponse:
        context = self._get_general_context(request.query)
        
        return AgentResponse(
            content="", # S·∫Ω ƒë∆∞·ª£c fill b·ªüi streaming
            confidence=0.8,
            agent_type=self.agent_type,
            metadata={'context_size': len(context), 'source': 'qa_script'}
        )
    
    def process_stream(self, request: TaskRequest):
        """Streaming version c·ªßa process method"""
        context = self._get_general_context(request.query)
        prompt = self.consultant_prompt.format(context="\n\n".join(context), query=request.query)
        
        yield from self._generate_stream_response(prompt)
    
    def _get_general_context(self, query: str, k: int = 3) -> List[str]:
        """L·∫•y context t·ª´ general Q&A vector store"""
        try:
            results = self.vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in results]
        except Exception as e:
            st.error(f"L·ªói khi truy xu·∫•t th√¥ng tin Q&A: {e}")
            return []

# --- Agent Manager (PHI√äN B·∫¢N T·ªîNG QU√ÅT) ---
class AgentManager:
    def __init__(self):
        self.model = genai.GenerativeModel(GENERATIVE_MODEL)
        self.product_store = load_or_create_product_faiss()
        self.script_store = load_or_create_script_faiss()

        self.router = RouterAgent(self.model)
        self.product_specialist = ProductSpecialistAgent(self.model, self.product_store)
        self.general_consultant = GeneralConsultantAgent(self.model, self.script_store)

        self.agents = {
            'product_specialist': self.product_specialist,
            'general_consultant': self.general_consultant
        }

    def _pre_route_check(self, query: str, threshold: float = 0.35) -> str:
        """
        Ki·ªÉm tra nhanh trong DB s·∫£n ph·∫©m ƒë·ªÉ t·∫°o g·ª£i √Ω cho Router.
        S·ª≠ d·ª•ng search_with_score ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô li√™n quan.
        (L∆∞u √Ω: FAISS tr·∫£ v·ªÅ kho·∫£ng c√°ch L2, ƒëi·ªÉm c√†ng th·∫•p c√†ng t·ªët)
        """
        try:
            # T√¨m 1 t√†i li·ªáu li√™n quan nh·∫•t v√† ƒëi·ªÉm s·ªë c·ªßa n√≥
            results = self.product_store.similarity_search_with_score(query, k=1)
            if results:
                top_doc, score = results[0]
                print(f"DEBUG: Pre-route check for '{query}' -> Top result score: {score}") # ƒê·ªÉ debug
                if score < threshold:
                    # N·∫øu ƒëi·ªÉm s·ªë ƒë·ªß t·ªët (ƒë·ªß g·∫ßn), ƒë√¢y l√† c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m
                    return "G·ª£i √Ω: D·ªØ li·ªáu s·∫£n ph·∫©m c√≥ ch·ª©a th√¥ng tin r·∫•t li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y. R·∫•t c√≥ th·ªÉ ƒë√¢y l√† c√¢u h·ªèi cho product_specialist."
        except Exception as e:
            print(f"ERROR in _pre_route_check: {e}")
            # B·ªè qua n·∫øu c√≥ l·ªói
        
        # M·∫∑c ƒë·ªãnh ho·∫∑c n·∫øu kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ƒë·ªß t·ªët
        return "G·ª£i √Ω: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m c·ª• th·ªÉ n√†o kh·ªõp v·ªõi c√¢u h·ªèi. R·∫•t c√≥ th·ªÉ ƒë√¢y l√† c√¢u h·ªèi cho general_consultant."

    def process_query(self, query: str) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω query v·ªõi b∆∞·ªõc Pre-routing Check.
        """
        # 1. Th·ª±c hi·ªán Pre-routing check ƒë·ªÉ t·∫°o g·ª£i √Ω <-- THAY ƒê·ªîI QUAN TR·ªåNG
        hint = self._pre_route_check(query)

        # 2. Router quy·∫øt ƒë·ªãnh agent, c√≥ s·ª≠ d·ª•ng "g·ª£i √Ω"
        request = TaskRequest(query=query)
        routing_decision = self.router.process(request, hint) # <-- Truy·ªÅn "hint" v√†o router

        # 3. L·∫•y target agent
        target_agent_name = routing_decision['target_agent']
        target_agent = self.agents.get(target_agent_name, self.general_consultant)

        return {
            'agent': target_agent,
            'routing_info': routing_decision,
            'request': request
        }

# --- FAISS Loading Functions (fixed caching issues) ---
@st.cache_resource
def load_or_create_product_faiss(_embedder):
    """T·∫£i ho·∫∑c t·∫°o FAISS index cho d·ªØ li·ªáu s·∫£n ph·∫©m."""
    if os.path.exists(PRODUCT_FAISS_PATH):
        print(f"INFO: ƒêang t·∫£i ch·ªâ m·ª•c s·∫£n ph·∫©m t·ª´ '{PRODUCT_FAISS_PATH}'...")
        return FAISS.load_local(PRODUCT_FAISS_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("ƒêang t·∫°o ch·ªâ m·ª•c s·∫£n ph·∫©m...")
    print("INFO: B·∫Øt ƒë·∫ßu t·∫°o ch·ªâ m·ª•c FAISS cho s·∫£n ph·∫©m...")
    
    try:
        df = pd.read_csv(PRODUCT_CSV_FILE, encoding='utf-8')
        df.columns = [col.strip() for col in df.columns]

        documents = []
        fields_to_chunk = [
            "C√îNG D·ª§NG", "TH√ÄNH PH·∫¶N", "CH·ªà ƒê·ªäNH",
            "CH·ªêNG CH·ªà ƒê·ªäNH", "C√ÅCH D√ôNG", "B·∫¢O QU·∫¢N", "L∆ØU √ù KHI S·ª¨ D·ª§NG"
        ]

        for _, row in df.iterrows():
            product_name = row.get("S·∫¢N PH·∫®M", "Kh√¥ng r√µ t√™n")
            for field in fields_to_chunk:
                if field in row and pd.notna(row[field]):
                    chunk_content = f"S·∫£n ph·∫©m: {product_name}\nTh√¥ng tin v·ªÅ '{field}': {row[field]}"
                    documents.append(chunk_content)

        if not documents:
            st.error("L·ªói: Kh√¥ng th·ªÉ t·∫°o ƒë∆∞·ª£c chunks t·ª´ file s·∫£n ph·∫©m.")
            st.stop()
        
        print(f"INFO: ƒê√£ t·∫°o ƒë∆∞·ª£c {len(documents)} chunks s·∫£n ph·∫©m.")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        vectorstore.save_local(PRODUCT_FAISS_PATH)
        print(f"INFO: L∆∞u ch·ªâ m·ª•c s·∫£n ph·∫©m th√†nh c√¥ng.")
        st.success("T·∫°o ch·ªâ m·ª•c s·∫£n ph·∫©m th√†nh c√¥ng!")
        return vectorstore
        
    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{PRODUCT_CSV_FILE}'.")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o ch·ªâ m·ª•c s·∫£n ph·∫©m: {e}")
        st.stop()

@st.cache_resource
def load_or_create_script_faiss(_embedder):
    """T·∫£i ho·∫∑c t·∫°o FAISS index cho k·ªãch b·∫£n Q&A."""
    if os.path.exists(SCRIPT_FAISS_PATH):
        print(f"INFO: ƒêang t·∫£i ch·ªâ m·ª•c k·ªãch b·∫£n t·ª´ '{SCRIPT_FAISS_PATH}'...")
        return FAISS.load_local(SCRIPT_FAISS_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("ƒêang t·∫°o ch·ªâ m·ª•c k·ªãch b·∫£n...")
    print("INFO: B·∫Øt ƒë·∫ßu t·∫°o ch·ªâ m·ª•c FAISS cho k·ªãch b·∫£n...")

    try:
        df = pd.read_csv(SCRIPT_CSV_FILE, encoding='utf-8')
        documents = [
            f"C√¢u h·ªèi: {row['C√¢u h·ªèi']}\nTr·∫£ l·ªùi: {row['Tr·∫£ l·ªùi']}"
            for _, row in df.iterrows() if pd.notna(row['C√¢u h·ªèi']) or pd.notna(row['Tr·∫£ l·ªùi'])
        ]
        
        if not documents:
            st.error("L·ªói: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu h·ª£p l·ªá trong file k·ªãch b·∫£n.")
            st.stop()
        
        print(f"INFO: ƒê√£ t·∫°o ƒë∆∞·ª£c {len(documents)} documents k·ªãch b·∫£n.")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        vectorstore.save_local(SCRIPT_FAISS_PATH)
        print(f"INFO: L∆∞u ch·ªâ m·ª•c k·ªãch b·∫£n th√†nh c√¥ng.")
        st.success("T·∫°o ch·ªâ m·ª•c k·ªãch b·∫£n th√†nh c√¥ng!")
        return vectorstore
        
    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{SCRIPT_CSV_FILE}'.")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o ch·ªâ m·ª•c k·ªãch b·∫£n: {e}")
        st.stop()

# --- C·∫•u h√¨nh API ---
try:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
except Exception:
    st.error("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p GEMINI_API_KEY trong Streamlit Secrets.")
    st.stop()

# --- Kh·ªüi T·∫°o Agent Manager ---
@st.cache_resource
def get_agent_manager():
    return AgentManager()

# --- STREAMLIT UI ---
st.set_page_config(page_title="EKS Agent-to-Agent System", page_icon="ü§ñ")
st.title("ü§ñ EKS Agent-to-Agent AI System")
st.caption("H·ªá th·ªëng AI ƒëa t√°c nh√¢n v·ªõi Router Agent v√† Task Agents chuy√™n bi·ªát")

# Sidebar th√¥ng tin agents
with st.sidebar:
    st.header("üéØ System Agents")
    st.write("**üß≠ Router Agent**: Ph√¢n t√≠ch v√† ƒëi·ªÅu ph·ªëi")
    st.write("**üë®‚Äçüî¨ Product Specialist**: Chuy√™n gia s·∫£n ph·∫©m")
    st.write("**üë®‚Äçüíº General Consultant**: T∆∞ v·∫•n vi√™n chung")
    
    st.header("üîç Debug Mode")
    show_routing = st.checkbox("Hi·ªÉn th·ªã th√¥ng tin routing")

# Kh·ªüi t·∫°o Agent Manager
try:
    agent_manager = get_agent_manager()
    st.success("‚úÖ Agent system initialized successfully!")
except Exception as e:
    st.error(f"‚ùå L·ªói kh·ªüi t·∫°o agent system: {e}")
    st.stop()

# Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": """
üéâ **Ch√†o m·ª´ng ƒë·∫øn v·ªõi EKS Agent-to-Agent System!**

**H·ªá th·ªëng c√≥ 3 AI Agents:**
- üß≠ **Router Agent**: Ph√¢n t√≠ch c√¢u h·ªèi v√† ƒëi·ªÅu ph·ªëi
- üë®‚Äçüî¨ **Product Specialist**: Chuy√™n gia chi ti·∫øt v·ªÅ s·∫£n ph·∫©m
- üë®‚Äçüíº **General Consultant**: T∆∞ v·∫•n vi√™n cho c√¢u h·ªèi chung

**B·∫°n c√≥ th·ªÉ h·ªèi:**
- Chi ti·∫øt v·ªÅ s·∫£n ph·∫©m (th√†nh ph·∫ßn, c√¥ng d·ª•ng, c√°ch d√πng...)
- So s√°nh s·∫£n ph·∫©m, th√¥ng tin th∆∞∆°ng hi·ªáu
- Ch√≠nh s√°ch, d·ªãch v·ª•, ƒë·ªãa ch·ªâ mua h√†ng

H√£y ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ tr·∫£i nghi·ªám h·ªá th·ªëng AI th√¥ng minh! üòä
        """}
    ]

# Hi·ªÉn th·ªã chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
if prompt := st.chat_input("H·ªèi EKS Agent System..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Process with Agent-to-Agent system
    with st.chat_message("assistant"):
        with st.spinner("üß≠ Router Agent ƒëang ph√¢n t√≠ch..."):
            # 1. Get routing decision
            processing_result = agent_manager.process_query(prompt)
            agent = processing_result['agent']
            routing_info = processing_result['routing_info']
            request = processing_result['request']
            
            # 2. Show routing info if debug enabled
            if show_routing:
                with st.expander("üîç Agent Routing Information"):
                    st.json({
                        "selected_agent": routing_info['target_agent'],
                        "confidence": routing_info['confidence'],
                        "reasoning": routing_info['reasoning']
                    })
            
            # 3. Display which agent is responding
            agent_emoji = "üë®‚Äçüî¨" if agent.agent_type == AgentType.PRODUCT_SPECIALIST else "üë®‚Äçüíº"
            agent_name = "Product Specialist" if agent.agent_type == AgentType.PRODUCT_SPECIALIST else "General Consultant"
            
            st.write(f"{agent_emoji} **{agent_name} Agent** ƒëang x·ª≠ l√Ω...")
            
            # 4. Stream response from selected agent
            response_generator = agent.process_stream(request)
            full_response = st.write_stream(response_generator)

    # Add assistant response to history
    st.session_state.messages.append({"role": "assistant", "content": full_response})