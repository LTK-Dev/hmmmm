import streamlit as st
import pandas as pd
import os
import google.generativeai as genai
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
import re

# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho FAISS vÃ  LangChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate

# --- 1. Tá»I Æ¯U HIá»†U NÄ‚NG Vá»šI CACHING ---
@st.cache_resource
def get_embedder():
    """Táº£i vÃ  cache mÃ´ hÃ¬nh embedding."""
    print("INFO: Äang táº£i mÃ´ hÃ¬nh embedding...")
    return SentenceTransformerEmbeddings(
        model_name=EMBEDDER_MODEL,
        model_kwargs={'device': 'cpu'}
    )

# --- Data Classes vÃ  Enums ---
class SourceType(Enum):
    SCRIPT = "script"
    PRODUCT = "product"
    HYBRID = "hybrid"

@dataclass
class RetrievedInfo:
    content: str
    source_type: SourceType
    score: float
    metadata: Dict[str, Any] = None

@dataclass
class MasterDecision:
    primary_source: SourceType
    confidence: float
    reasoning: str
    selected_info: List[RetrievedInfo]
    response_strategy: str

@dataclass
class TaskRequest:
    query: str
    context: List[str] = None
    metadata: Dict[str, Any] = None

# --- Cáº¥u hÃ¬nh ---
PRODUCT_CSV_FILE = 'EKS.csv'
SCRIPT_CSV_FILE = 'EKS_ques.csv'
PRODUCT_FAISS_PATH = "faiss_index_product"
SCRIPT_FAISS_PATH = "faiss_index_script"
EMBEDDER_MODEL = 'intfloat/multilingual-e5-base'
GENERATIVE_MODEL = 'gemini-2.0-flash'

# --- Master Agent (Thay tháº¿ Router) ---
class MasterAgent:
    def __init__(self, model: genai.GenerativeModel, product_store: FAISS, script_store: FAISS):
        self.model = model
        self.product_store = product_store
        self.script_store = script_store
        
        self.evaluation_prompt = """
Báº¡n lÃ  Master Agent cá»§a há»‡ thá»‘ng EKS - cÃ³ nhiá»‡m vá»¥ Ä‘Ã¡nh giÃ¡ vÃ  quyáº¿t Ä‘á»‹nh nguá»“n thÃ´ng tin tá»‘t nháº¥t Ä‘á»ƒ tráº£ lá»i khÃ¡ch hÃ ng.

**CÃ¢u há»i tá»« khÃ¡ch hÃ ng:**
{query}

**THÃ”NG TIN Tá»ª Ká»ŠCH Báº¢N Q&A:**
{script_info}

**THÃ”NG TIN Tá»ª DATABASE Sáº¢N PHáº¨M:**
{product_info}

**NHIá»†M Vá»¤ Cá»¦A Báº N:**
1. ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  Ä‘á»™ phÃ¹ há»£p cá»§a má»—i nguá»“n thÃ´ng tin
2. Quyáº¿t Ä‘á»‹nh nguá»“n nÃ o nÃªn Ä‘Æ°á»£c Æ°u tiÃªn
3. ÄÆ°a ra chiáº¿n lÆ°á»£c tráº£ lá»i phÃ¹ há»£p

**QUY Táº®C Æ¯U TIÃŠN:**
- **Ká»ŠCH Báº¢N Q&A**: Æ¯u tiÃªn cao nháº¥t náº¿u cÃ³ cÃ¢u tráº£ lá»i trá»±c tiáº¿p vÃ  chÃ­nh xÃ¡c
- **Sáº¢N PHáº¨M**: Sá»­ dá»¥ng khi cáº§n thÃ´ng tin chi tiáº¿t, ká»¹ thuáº­t vá» sáº£n pháº©m
- **Káº¾T Há»¢P**: DÃ¹ng cáº£ hai nguá»“n khi cáº§n thÃ´ng tin toÃ n diá»‡n

Tráº£ lá»i theo format JSON:
{{
    "primary_source": "script/product/hybrid",
    "confidence": 0.9,
    "reasoning": "LÃ½ do chi tiáº¿t vá» quyáº¿t Ä‘á»‹nh",
    "response_strategy": "Chiáº¿n lÆ°á»£c tráº£ lá»i cá»¥ thá»ƒ"
}}
"""

        self.response_prompt = """
Báº¡n lÃ  EKS Master Agent - chuyÃªn gia tÆ° váº¥n hÃ ng Ä‘áº§u vá» sáº£n pháº©m má»¹ pháº©m EKS.

**CÃ¢u há»i tá»« khÃ¡ch hÃ ng:**
{query}

**THÃ”NG TIN ÄÃƒ ÄÆ¯á»¢C CHá»ŒN:**
{selected_info}

**CHIáº¾N LÆ¯á»¢C TRáº¢ Lá»œI:**
{strategy}

**HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:**
1. **XÆ°ng hÃ´**: "TÃ´i lÃ  EKS Master Agent"
2. **Æ¯u tiÃªn ká»‹ch báº£n**: Náº¿u cÃ³ thÃ´ng tin tá»« ká»‹ch báº£n Q&A, sá»­ dá»¥ng y nguyÃªn
3. **Bá»• sung sáº£n pháº©m**: ThÃªm chi tiáº¿t ká»¹ thuáº­t tá»« database sáº£n pháº©m náº¿u cáº§n
4. **Phong cÃ¡ch**: ChuyÃªn nghiá»‡p, thÃ¢n thiá»‡n, dá»… hiá»ƒu
5. **Cáº¥u trÃºc**: RÃµ rÃ ng, cÃ³ logic, dá»… theo dÃµi

**LÆ¯U Ã QUAN TRá»ŒNG:**
- LuÃ´n dá»±a trÃªn thÃ´ng tin cÃ³ sáºµn
- Thá»«a nháº­n náº¿u khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin
- ÄÆ°a ra lá»i khuyÃªn thá»±c táº¿ vÃ  há»¯u Ã­ch
- Náº¿u trong ká»‹ch báº£n Q&A tháº­t sá»± cÃ³ cÃ¢u há»i cÃ³ hÃ m Ã½ vÃ  cÃ¢u tráº£ lá»i liÃªn quan Ä‘áº¿n query cá»§a khÃ¡ch hÃ ng, hÃ£y sá»­ dá»¥ng nguyÃªn cÃ¢u tráº£ lá»i Ä‘Ã³, Ä‘á»«ng Æ°u tiÃ©n tÆ° váº¥n quÃ¡ nhiá»u.
- Chá»‰ khi mÃ  ká»‹ch báº£n tháº­t sá»± khÃ´ng cÃ³ cÃ¢u tráº£ lá»i nÃ o liÃªn quan Ä‘áº¿n query cá»§a khÃ¡ch hÃ ng, hÃ£y sá»­ dá»¥ng thÃ´ng tin tá»« database sáº£n pháº©m Ä‘á»ƒ tÆ° váº¥n.
- KhÃ¡ch hÃ ng Æ°u tiÃªn cÃ¢u tráº£ lá»i ngáº¯n gá»n vÃ  sÃºc tÃ­ch, Ä‘i vÃ o trá»ng tÃ¢m váº¥n Ä‘á» rá»“i má»›i diá»…n giáº£i chi tiáº¿t náº¿u cáº§n thiáº¿t.
"""

    def retrieve_all_sources(self, query: str, k_script: int = 3, k_product: int = 5) -> Tuple[List[RetrievedInfo], List[RetrievedInfo]]:
        """Truy váº¥n thÃ´ng tin tá»« cáº£ hai vector database."""
        script_infos = []
        product_infos = []
        
        try:
            # Truy váº¥n ká»‹ch báº£n Q&A
            script_results = self.script_store.similarity_search_with_score(query, k=k_script)
            for doc, score in script_results:
                script_infos.append(RetrievedInfo(
                    content=doc.page_content,
                    source_type=SourceType.SCRIPT,
                    score=score,
                    metadata={'source': 'qa_script'}
                ))
            
            # Truy váº¥n database sáº£n pháº©m
            product_results = self.product_store.similarity_search_with_score(query, k=k_product)
            for doc, score in product_results:
                product_infos.append(RetrievedInfo(
                    content=doc.page_content,
                    source_type=SourceType.PRODUCT,
                    score=score,
                    metadata={'source': 'product_db'}
                ))
                
        except Exception as e:
            print(f"ERROR: Lá»—i khi truy váº¥n vector stores: {e}")
            
        return script_infos, product_infos

    def evaluate_and_decide(self, query: str, script_infos: List[RetrievedInfo], product_infos: List[RetrievedInfo]) -> MasterDecision:
        """ÄÃ¡nh giÃ¡ vÃ  quyáº¿t Ä‘á»‹nh nguá»“n thÃ´ng tin tá»‘t nháº¥t."""
        
        # Chuáº©n bá»‹ thÃ´ng tin cho prompt
        script_content = "\n\n".join([f"Score: {info.score:.3f}\n{info.content}" for info in script_infos[:3]])
        product_content = "\n\n".join([f"Score: {info.score:.3f}\n{info.content}" for info in product_infos[:3]])
        
        if not script_content:
            script_content = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong ká»‹ch báº£n Q&A"
        if not product_content:
            product_content = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong database sáº£n pháº©m"
        
        try:
            prompt = self.evaluation_prompt.format(
                query=query,
                script_info=script_content,
                product_info=product_content
            )
            
            response = self.model.generate_content(prompt)
            decision_text = self._extract_json_from_response(response.text)
            
            if decision_text:
                decision_data = json.loads(decision_text)
                
                # Chá»n thÃ´ng tin dá»±a trÃªn quyáº¿t Ä‘á»‹nh
                selected_info = self._select_info_based_on_decision(
                    decision_data.get('primary_source', 'script'),
                    script_infos,
                    product_infos
                )
                
                return MasterDecision(
                    primary_source=SourceType(decision_data.get('primary_source', 'script')),
                    confidence=decision_data.get('confidence', 0.7),
                    reasoning=decision_data.get('reasoning', 'Quyáº¿t Ä‘á»‹nh tá»± Ä‘á»™ng'),
                    selected_info=selected_info,
                    response_strategy=decision_data.get('response_strategy', 'Tráº£ lá»i dá»±a trÃªn thÃ´ng tin cÃ³ sáºµn')
                )
                
        except Exception as e:
            print(f"WARNING: Lá»—i khi Ä‘Ã¡nh giÃ¡, sá»­ dá»¥ng fallback logic: {e}")
            
        # Fallback logic
        return self._fallback_decision(query, script_infos, product_infos)

    def _extract_json_from_response(self, text: str) -> Optional[str]:
        """TrÃ­ch xuáº¥t JSON tá»« response."""
        try:
            # LÃ m sáº¡ch text
            cleaned_text = text.strip()
            if "```json" in cleaned_text:
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', cleaned_text, re.DOTALL)
                if json_match:
                    return json_match.group(1)
            
            # TÃ¬m JSON object
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_text)
            if json_match:
                return json_match.group(0)
                
            return None
        except Exception as e:
            print(f"WARNING: Lá»—i extract JSON: {e}")
            return None

    def _select_info_based_on_decision(self, primary_source: str, script_infos: List[RetrievedInfo], product_infos: List[RetrievedInfo]) -> List[RetrievedInfo]:
        """Chá»n thÃ´ng tin dá»±a trÃªn quyáº¿t Ä‘á»‹nh cá»§a Master Agent."""
        if primary_source == 'script':
            return script_infos[:2] + product_infos[:1]  # Æ¯u tiÃªn script
        elif primary_source == 'product':
            return product_infos[:3] + script_infos[:1]  # Æ¯u tiÃªn product
        else:  # hybrid
            return script_infos[:2] + product_infos[:2]  # CÃ¢n báº±ng

    def _fallback_decision(self, query: str, script_infos: List[RetrievedInfo], product_infos: List[RetrievedInfo]) -> MasterDecision:
        """Logic fallback khi khÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c."""
        
        # TÃ­nh Ä‘iá»ƒm trung bÃ¬nh cho má»—i nguá»“n
        script_avg_score = sum(info.score for info in script_infos[:2]) / max(len(script_infos[:2]), 1) if script_infos else 1.0
        product_avg_score = sum(info.score for info in product_infos[:2]) / max(len(product_infos[:2]), 1) if product_infos else 1.0
        
        # FAISS sá»­ dá»¥ng L2 distance, score tháº¥p = tá»‘t hÆ¡n
        if script_infos and script_avg_score < 0.4:  # Script cÃ³ káº¿t quáº£ tá»‘t
            return MasterDecision(
                primary_source=SourceType.SCRIPT,
                confidence=0.7,
                reasoning="Fallback: Ká»‹ch báº£n Q&A cÃ³ thÃ´ng tin phÃ¹ há»£p",
                selected_info=script_infos[:2] + product_infos[:1],
                response_strategy="Æ¯u tiÃªn cÃ¢u tráº£ lá»i tá»« ká»‹ch báº£n, bá»• sung thÃ´ng tin sáº£n pháº©m"
            )
        elif product_infos and product_avg_score < 0.5:  # Product cÃ³ káº¿t quáº£ khÃ¡ tá»‘t
            return MasterDecision(
                primary_source=SourceType.PRODUCT,
                confidence=0.6,
                reasoning="Fallback: Database sáº£n pháº©m cÃ³ thÃ´ng tin liÃªn quan",
                selected_info=product_infos[:3],
                response_strategy="Táº­p trung vÃ o thÃ´ng tin chi tiáº¿t sáº£n pháº©m"
            )
        else:  # Káº¿t há»£p cáº£ hai
            return MasterDecision(
                primary_source=SourceType.HYBRID,
                confidence=0.5,
                reasoning="Fallback: Káº¿t há»£p thÃ´ng tin tá»« cáº£ hai nguá»“n",
                selected_info=script_infos[:1] + product_infos[:2],
                response_strategy="Káº¿t há»£p thÃ´ng tin tá»« ká»‹ch báº£n vÃ  sáº£n pháº©m"
            )

    def generate_response_stream(self, query: str, decision: MasterDecision):
        """Generate streaming response dá»±a trÃªn quyáº¿t Ä‘á»‹nh cá»§a Master Agent."""
        
        # Chuáº©n bá»‹ thÃ´ng tin Ä‘Ã£ chá»n
        selected_content = "\n\n".join([
            f"[{info.source_type.value.upper()}] {info.content}" 
            for info in decision.selected_info
        ])
        
        prompt = self.response_prompt.format(
            query=query,
            selected_info=selected_content,
            strategy=decision.response_strategy
        )
        
        try:
            response_stream = self.model.generate_content(prompt, stream=True)
            for chunk in response_stream:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            st.error(f"Lá»—i khi generate response: {e}")
            yield f"Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n."

# --- Agent Manager Ä‘Æ°á»£c cáº­p nháº­t ---
class AgentManager:
    def __init__(self):
        self.model = genai.GenerativeModel(GENERATIVE_MODEL)
        embedder = get_embedder()
        self.product_store = load_or_create_product_faiss(embedder)
        self.script_store = load_or_create_script_faiss(embedder)
        
        # Khá»Ÿi táº¡o Master Agent thay vÃ¬ Router
        self.master_agent = MasterAgent(self.model, self.product_store, self.script_store)

    def process_query(self, query: str) -> Dict[str, Any]:
        """Xá»­ lÃ½ query vá»›i Master Agent."""
        
        # 1. Truy váº¥n táº¥t cáº£ nguá»“n thÃ´ng tin
        script_infos, product_infos = self.master_agent.retrieve_all_sources(query)
        
        # 2. Master Agent Ä‘Ã¡nh giÃ¡ vÃ  quyáº¿t Ä‘á»‹nh
        decision = self.master_agent.evaluate_and_decide(query, script_infos, product_infos)
        
        return {
            'master_decision': decision,
            'script_infos': script_infos,
            'product_infos': product_infos,
            'query': query
        }

    def get_response_stream(self, query: str, decision: MasterDecision):
        """Láº¥y streaming response tá»« Master Agent."""
        return self.master_agent.generate_response_stream(query, decision)

# --- FAISS Loading Functions (giá»¯ nguyÃªn) ---
@st.cache_resource
def load_or_create_product_faiss(_embedder):
    """Táº£i hoáº·c táº¡o FAISS index cho dá»¯ liá»‡u sáº£n pháº©m."""
    if os.path.exists(PRODUCT_FAISS_PATH):
        print(f"INFO: Äang táº£i chá»‰ má»¥c sáº£n pháº©m tá»« '{PRODUCT_FAISS_PATH}'...")
        return FAISS.load_local(PRODUCT_FAISS_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("Äang táº¡o chá»‰ má»¥c sáº£n pháº©m...")
    print("INFO: Báº¯t Ä‘áº§u táº¡o chá»‰ má»¥c FAISS cho sáº£n pháº©m...")
    
    try:
        df = pd.read_csv(PRODUCT_CSV_FILE, encoding='utf-8')
        df.columns = [col.strip() for col in df.columns]

        documents = []
        fields_to_chunk = [
            "CÃ”NG Dá»¤NG", "THÃ€NH PHáº¦N", "CHá»ˆ Äá»ŠNH",
            "CHá»NG CHá»ˆ Äá»ŠNH", "CÃCH DÃ™NG", "Báº¢O QUáº¢N", "LÆ¯U Ã KHI Sá»¬ Dá»¤NG"
        ]

        for _, row in df.iterrows():
            product_name = row.get("Sáº¢N PHáº¨M", "KhÃ´ng rÃµ tÃªn")
            for field in fields_to_chunk:
                if field in row and pd.notna(row[field]):
                    chunk_content = f"Sáº£n pháº©m: {product_name}\nThÃ´ng tin vá» '{field}': {row[field]}"
                    documents.append(chunk_content)

        if not documents:
            st.error("Lá»—i: KhÃ´ng thá»ƒ táº¡o Ä‘Æ°á»£c chunks tá»« file sáº£n pháº©m.")
            st.stop()
        
        print(f"INFO: ÄÃ£ táº¡o Ä‘Æ°á»£c {len(documents)} chunks sáº£n pháº©m.")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        vectorstore.save_local(PRODUCT_FAISS_PATH)
        print(f"INFO: LÆ°u chá»‰ má»¥c sáº£n pháº©m thÃ nh cÃ´ng.")
        st.success("Táº¡o chá»‰ má»¥c sáº£n pháº©m thÃ nh cÃ´ng!")
        return vectorstore
        
    except FileNotFoundError:
        st.error(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{PRODUCT_CSV_FILE}'.")
        st.stop()
    except Exception as e:
        st.error(f"Lá»—i khi táº¡o chá»‰ má»¥c sáº£n pháº©m: {e}")
        st.stop()

@st.cache_resource
def load_or_create_script_faiss(_embedder):
    """Táº£i hoáº·c táº¡o FAISS index cho ká»‹ch báº£n Q&A."""
    if os.path.exists(SCRIPT_FAISS_PATH):
        print(f"INFO: Äang táº£i chá»‰ má»¥c ká»‹ch báº£n tá»« '{SCRIPT_FAISS_PATH}'...")
        return FAISS.load_local(SCRIPT_FAISS_PATH, _embedder, allow_dangerous_deserialization=True)

    st.info("Äang táº¡o chá»‰ má»¥c ká»‹ch báº£n...")
    print("INFO: Báº¯t Ä‘áº§u táº¡o chá»‰ má»¥c FAISS cho ká»‹ch báº£n...")

    try:
        df = pd.read_csv(SCRIPT_CSV_FILE, encoding='utf-8')
        documents = [
            f"CÃ¢u há»i: {row['CÃ¢u há»i']}\nTráº£ lá»i: {row['Tráº£ lá»i']}"
            for _, row in df.iterrows() if pd.notna(row['CÃ¢u há»i']) or pd.notna(row['Tráº£ lá»i'])
        ]
        
        if not documents:
            st.error("Lá»—i: KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u há»£p lá»‡ trong file ká»‹ch báº£n.")
            st.stop()
        
        print(f"INFO: ÄÃ£ táº¡o Ä‘Æ°á»£c {len(documents)} documents ká»‹ch báº£n.")
        vectorstore = FAISS.from_texts(texts=documents, embedding=_embedder)
        vectorstore.save_local(SCRIPT_FAISS_PATH)
        print(f"INFO: LÆ°u chá»‰ má»¥c ká»‹ch báº£n thÃ nh cÃ´ng.")
        st.success("Táº¡o chá»‰ má»¥c ká»‹ch báº£n thÃ nh cÃ´ng!")
        return vectorstore
        
    except FileNotFoundError:
        st.error(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{SCRIPT_CSV_FILE}'.")
        st.stop()
    except Exception as e:
        st.error(f"Lá»—i khi táº¡o chá»‰ má»¥c ká»‹ch báº£n: {e}")
        st.stop()

# --- Cáº¥u hÃ¬nh API ---
try:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
except Exception:
    st.error("Lá»—i: Vui lÃ²ng thiáº¿t láº­p GEMINI_API_KEY trong Streamlit Secrets.")
    st.stop()

# --- Khá»Ÿi Táº¡o Agent Manager ---
@st.cache_resource
def get_agent_manager():
    return AgentManager()

# --- STREAMLIT UI ---
st.set_page_config(page_title="EKS Agentic RAG System", page_icon="ğŸ¤–")
st.title("ğŸ¤– EKS Agentic RAG System")
st.caption("Há»‡ thá»‘ng AI Master Agent vá»›i RAG thÃ´ng minh tá»« nhiá»u nguá»“n dá»¯ liá»‡u")

# Sidebar thÃ´ng tin há»‡ thá»‘ng
with st.sidebar:
    st.header("ğŸ¯ Agentic RAG System")
    st.write("**ğŸ§  Master Agent**: ÄÃ¡nh giÃ¡ vÃ  quyáº¿t Ä‘á»‹nh nguá»“n thÃ´ng tin")
    st.write("**ğŸ“š Script Database**: Ká»‹ch báº£n Q&A cÃ³ sáºµn")
    st.write("**ğŸ›ï¸ Product Database**: Chi tiáº¿t sáº£n pháº©m EKS")
    
    st.header("ğŸ” Debug Mode")
    show_decision = st.checkbox("Hiá»ƒn thá»‹ quyáº¿t Ä‘á»‹nh Master Agent")
    show_sources = st.checkbox("Hiá»ƒn thá»‹ nguá»“n thÃ´ng tin")

# Khá»Ÿi táº¡o Agent Manager
try:
    agent_manager = get_agent_manager()
    st.success("âœ… Agentic RAG system initialized successfully!")
except Exception as e:
    st.error(f"âŒ Lá»—i khá»Ÿi táº¡o agentic RAG system: {e}")
    st.stop()

# Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": """
ğŸ‰ **ChÃ o má»«ng Ä‘áº¿n vá»›i EKS Agentic RAG System!**

**Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o:**
- ğŸ§  **Master Agent** truy váº¥n Ä‘á»“ng thá»i cáº£ database ká»‹ch báº£n vÃ  sáº£n pháº©m
- ğŸ¯ Tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ vÃ  chá»n nguá»“n thÃ´ng tin tá»‘t nháº¥t
- ğŸ“‹ Æ¯u tiÃªn cÃ¢u tráº£ lá»i tá»« ká»‹ch báº£n Q&A náº¿u phÃ¹ há»£p
- ğŸ”¬ Bá»• sung thÃ´ng tin chi tiáº¿t tá»« database sáº£n pháº©m khi cáº§n

**Báº¡n cÃ³ thá»ƒ há»i báº¥t ká»³ Ä‘iá»u gÃ¬ vá»:**
- Sáº£n pháº©m EKS (thÃ nh pháº§n, cÃ´ng dá»¥ng, cÃ¡ch dÃ¹ng...)
- ChÃ­nh sÃ¡ch, dá»‹ch vá»¥, hÆ°á»›ng dáº«n mua hÃ ng
- So sÃ¡nh vÃ  tÆ° váº¥n lá»±a chá»n sáº£n pháº©m

HÃ£y thá»­ há»i Ä‘á»ƒ tráº£i nghiá»‡m sá»©c máº¡nh cá»§a Agentic RAG! ğŸš€
        """}
    ]

# Hiá»ƒn thá»‹ chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
if prompt := st.chat_input("Há»i EKS Agentic RAG System..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Process with Agentic RAG system
    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  Master Agent Ä‘ang phÃ¢n tÃ­ch vÃ  truy váº¥n dá»¯ liá»‡u..."):
            # 1. Process query vá»›i Master Agent
            processing_result = agent_manager.process_query(prompt)
            decision = processing_result['master_decision']
            script_infos = processing_result['script_infos']
            product_infos = processing_result['product_infos']
            
            # 2. Show decision info if debug enabled
            if show_decision:
                with st.expander("ğŸ§  Master Agent Decision"):
                    st.json({
                        "primary_source": decision.primary_source.value,
                        "confidence": decision.confidence,
                        "reasoning": decision.reasoning,
                        "response_strategy": decision.response_strategy,
                        "selected_info_count": len(decision.selected_info)
                    })
            
            # 3. Show sources if debug enabled
            if show_sources:
                col1, col2 = st.columns(2)
                with col1:
                    with st.expander("ğŸ“š Script Sources"):
                        for i, info in enumerate(script_infos[:3]):
                            st.write(f"**Score:** {info.score:.3f}")
                            st.write(info.content[:200] + "..." if len(info.content) > 200 else info.content)
                            st.divider()
                
                with col2:
                    with st.expander("ğŸ›ï¸ Product Sources"):
                        for i, info in enumerate(product_infos[:3]):
                            st.write(f"**Score:** {info.score:.3f}")
                            st.write(info.content[:200] + "..." if len(info.content) > 200 else info.content)
                            st.divider()
            
            # 4. Display Master Agent response strategy
            strategy_emoji = {
                SourceType.SCRIPT: "ğŸ“š",
                SourceType.PRODUCT: "ğŸ›ï¸", 
                SourceType.HYBRID: "ğŸ”„"
            }
            
            emoji = strategy_emoji.get(decision.primary_source, "ğŸ¤–")
            st.write(f"{emoji} **Master Agent** (Strategy: {decision.primary_source.value}) Ä‘ang tráº£ lá»i...")
            
            # 5. Stream response from Master Agent
            response_generator = agent_manager.get_response_stream(prompt, decision)
            full_response = st.write_stream(response_generator)

    # Add assistant response to history
    st.session_state.messages.append({"role": "assistant", "content": full_response})